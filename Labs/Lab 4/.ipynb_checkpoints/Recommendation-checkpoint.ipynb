{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  3.  2.  3.]\n",
      " [ 1.  2.  3.  1.]\n",
      " [nan  2.  1. nan]\n",
      " [ 4.  3. nan nan]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: load dataset into variable M\n",
    "M = np.array([[4, 3, 2, 3],\n",
    "               [1, 2, 3, 1],\n",
    "               [np.nan, 2, 1, np.nan],\n",
    "               [4, 3, np.nan, np.nan]])\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(v1,v2, metric='cosine'):\n",
    "    #metric: cosine or correlation\n",
    "    if metric == 'correlation':\n",
    "        v1 = v1 - np.nanmean(v1)\n",
    "        v2 = v2 - np.nanmean(v2)\n",
    "    \"compute similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        if np.isnan(x) or np.isnan(y): continue\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def sim_matrix(M, dimension='user', metric='cosine'):\n",
    "    N = M.shape[0] if dimension == 'user' else M.shape[1]\n",
    "    sim = np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                sim[i,j] = 0 #Cancel out the effect of self-similarity in the sums later\n",
    "                continue\n",
    "            if dimension == 'user':\n",
    "                v1, v2 = M[i,:], M[j,:]\n",
    "            else:\n",
    "                v1, v2 = M[:,i], M[:,j]\n",
    "            sim[i][j] = cosine_similarity(v1,v2,metric)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79582243, 0.99227788, 1.        ],\n",
       "       [0.79582243, 0.        , 0.86824314, 0.89442719],\n",
       "       [0.99227788, 0.86824314, 0.        , 1.        ],\n",
       "       [1.        , 0.89442719, 1.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.9649505 , 0.73994007, 0.99705449],\n",
       "       [0.9649505 , 0.        , 0.90748521, 0.96476382],\n",
       "       [0.73994007, 0.90748521, 0.        , 0.78935222],\n",
       "       [0.99705449, 0.96476382, 0.78935222, 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.85280287,  0.70710678,  0.70710678],\n",
       "       [-0.85280287,  0.        , -0.5547002 , -0.89442719],\n",
       "       [ 0.70710678, -0.5547002 ,  0.        , -1.        ],\n",
       "       [ 0.70710678, -0.89442719, -1.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.94280904, -0.89442719,  0.9486833 ],\n",
       "       [ 0.94280904,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.89442719,  0.        ,  0.        , -0.70710678],\n",
       "       [ 0.9486833 ,  1.        , -0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute the missing rating in this table using user-based collaborative filtering (CF). (Use cosine similarity, then use Pearson similarity). Assume taking all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(M, u, i, simularity, avg, n_users):\n",
    "    total_sum = 0\n",
    "    bot_sum = 0\n",
    "    for v in range(n_users):\n",
    "        if (u != v):\n",
    "            bot_sum += simularity[u, v]\n",
    "    for v in range(n_users):\n",
    "        if (u != v and not np.isnan(M[v,i])):\n",
    "            total_sum += simularity[u, v]*(M[v, i]-avg[v])/bot_sum\n",
    "    return avg[u]+total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=1)\n",
    "    sim_users = sim_matrix(M, 'user', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                pred[i,j] = predict_user(M, i, j, sim_users, avg_ratings, n_users)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF (Cosine): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  1.794036  2.0  1.000000  1.272355\n",
      "3  4.000000  3.0  3.368034  3.268237\n",
      "User-based CF (Pearson): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  0.764822  2.0  1.000000  1.009169\n",
      "3  4.000000  3.0  4.616077  2.935013\n"
     ]
    }
   ],
   "source": [
    "user_cosine = user_cf(M, 'cosine')\n",
    "user_correlation = user_cf(M, 'correlation')\n",
    "\n",
    "print(\"User-based CF (Cosine): \\n\" + str(pd.DataFrame(user_cosine)))\n",
    "print(\"User-based CF (Pearson): \\n\" + str(pd.DataFrame(user_correlation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Similarly, computing the missing rating using item-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_item(M, u, i, simularity, avg, n_items):\n",
    "    total_sum = 0\n",
    "    bot_sum = 0\n",
    "    for j in range(n_items):\n",
    "        if (i != j):\n",
    "            bot_sum += simularity[i, j]\n",
    "    for j in range(n_items):\n",
    "        if (i != j and not np.isnan(M[u, j])):\n",
    "            total_sum += simularity[i, j]*(M[u, j]-avg[j])/bot_sum\n",
    "    return avg[i]+total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=0)\n",
    "    sim_items = sim_matrix(M, 'item', metric)\n",
    "    for u in range(n_users):\n",
    "        for i in range(n_items):\n",
    "            if np.isnan(M[u,i]):\n",
    "                pred[u,i] = predict_item(M, u, i, sim_items, avg_ratings, n_items)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF (Cosine): \n",
      "         0    1         2         3\n",
      "0  4.00000  3.0  2.000000  3.000000\n",
      "1  1.00000  2.0  3.000000  1.000000\n",
      "2  2.54758  2.0  1.000000  1.537748\n",
      "3  4.00000  3.0  2.489861  2.537748\n",
      "Item-based CF (Pearson): \n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  3.424268  2.0  1.000000  2.16681\n",
      "3  4.000000  3.0  2.558482  3.16681\n"
     ]
    }
   ],
   "source": [
    "item_cosine = item_cf(M, 'cosine')\n",
    "item_correlation = item_cf(M, 'correlation')\n",
    "\n",
    "print(\"Item-based CF (Cosine): \\n\" + str(pd.DataFrame(item_cosine)))\n",
    "print(\"Item-based CF (Pearson): \\n\" + str(pd.DataFrame(item_correlation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Recommendation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  4  3  2  3\n",
       "1  1  2  3  1\n",
       "2  1  2  1  2\n",
       "3  4  3  2  4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_result = np.asarray([[4,3,2,3], \n",
    "                [1,2,3,1],\n",
    "                [1,2,1,2],\n",
    "                [4,3,2,4]])\n",
    "pd.DataFrame(M_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRS(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #method: user_cf and item_cf, metric: cosine and correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    MSE = mean_squared_error(prediction, groundtruth)\n",
    "    RMSE = round(math.sqrt(MSE),3)\n",
    "    print(\"RMSE using {0} approach ({2}) is: {1}\".format(method, RMSE, metric))\n",
    "    print(pd.DataFrame(prediction))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using user_cf approach (cosine) is: 0.472\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  1.794036  2.0  1.000000  1.272355\n",
      "3  4.000000  3.0  3.368034  3.268237\n",
      "RMSE using user_cf approach (correlation) is: 0.751\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  0.764822  2.0  1.000000  1.009169\n",
      "3  4.000000  3.0  4.616077  2.935013\n",
      "RMSE using item_cf approach (cosine) is: 0.558\n",
      "         0    1         2         3\n",
      "0  4.00000  3.0  2.000000  3.000000\n",
      "1  1.00000  2.0  3.000000  1.000000\n",
      "2  2.54758  2.0  1.000000  1.537748\n",
      "3  4.00000  3.0  2.489861  2.537748\n",
      "RMSE using item_cf approach (correlation) is: 0.657\n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  3.424268  2.0  1.000000  2.16681\n",
      "3  4.000000  3.0  2.558482  3.16681\n"
     ]
    }
   ],
   "source": [
    "evaluateRS(M, M_result, 'user_cf', 'cosine')\n",
    "evaluateRS(M, M_result, 'user_cf', 'correlation')\n",
    "evaluateRS(M, M_result, 'item_cf', 'cosine')\n",
    "evaluateRS(M, M_result, 'item_cf', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def evaluate_rank(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    n_users, n_items = M.shape\n",
    "    #metric: cosine vs correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    \n",
    "    avg_tau = 0\n",
    "    for i in range(n_users):\n",
    "        tau, p_value = stats.kendalltau(M_result[i,:], prediction[i,:])\n",
    "        avg_tau += tau\n",
    "    avg_tau = avg_tau / n_users\n",
    "    return avg_tau\n",
    "\n",
    "\n",
    "#COMPLETE: calculate the ranking accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank accuracy of user with cosine metric  0.6477056190747297\n",
      "Rank accuracy of user with correlation metric  0.56719350585564\n",
      "Rank accuracy of item with cosine metric  0.6369306393762916\n",
      "Rank accuracy of item with correlation metric  0.7282177322938193\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank accuracy of user with cosine metric \", evaluate_rank(user_cosine, M_result, 'user_cf', 'cosine'))\n",
    "print(\"Rank accuracy of user with correlation metric \", evaluate_rank(user_correlation, M_result, 'user_cf', 'correlation'))\n",
    "print(\"Rank accuracy of item with cosine metric \", evaluate_rank(item_cosine, M_result, 'item_cf', 'cosine'))\n",
    "print(\"Rank accuracy of item with correlation metric \", evaluate_rank(item_correlation, M_result, 'item_cf', 'correlation'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
